{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import matplotlib\nimport numpy as np\nimport pandas as pd\nfrom collections import namedtuple\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nEpisodeStats = namedtuple(\"Stats\",[\"episode_lengths\", \"episode_rewards\"])\n\ndef plot_cost_to_go_mountain_car(env, estimator, num_tiles=20):\n    x = np.linspace(env.observation_space.low[0], env.observation_space.high[0], num=num_tiles)\n    y = np.linspace(env.observation_space.low[1], env.observation_space.high[1], num=num_tiles)\n    X, Y = np.meshgrid(x, y)\n    Z = np.apply_along_axis(lambda _: -np.max(estimator.predict(_)), 2, np.dstack([X, Y]))\n\n    fig = plt.figure(figsize=(10, 5))\n    ax = fig.add_subplot(111, projection='3d')\n    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n                           cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)\n    ax.set_xlabel('Position')\n    ax.set_ylabel('Velocity')\n    ax.set_zlabel('Value')\n    ax.set_title(\"Mountain \\\"Cost To Go\\\" Function\")\n    fig.colorbar(surf)\n    plt.show()\n\n\ndef plot_value_function(V, title=\"Value Function\"):\n    \"\"\"\n    Plots the value function as a surface plot.\n    \"\"\"\n    min_x = min(k[0] for k in V.keys())\n    max_x = max(k[0] for k in V.keys())\n    min_y = min(k[1] for k in V.keys())\n    max_y = max(k[1] for k in V.keys())\n\n    x_range = np.arange(min_x, max_x + 1)\n    y_range = np.arange(min_y, max_y + 1)\n    X, Y = np.meshgrid(x_range, y_range)\n\n    # Find value for all (x, y) coordinates\n    Z_noace = np.apply_along_axis(lambda _: V[(_[0], _[1], False)], 2, np.dstack([X, Y]))\n    Z_ace = np.apply_along_axis(lambda _: V[(_[0], _[1], True)], 2, np.dstack([X, Y]))\n\n    def plot_surface(X, Y, Z, title):\n        fig = plt.figure(figsize=(20, 10))\n        ax = fig.add_subplot(111, projection='3d')\n        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n                               cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)\n        ax.set_xlabel('Player Sum')\n        ax.set_ylabel('Dealer Showing')\n        ax.set_zlabel('Value')\n        ax.set_title(title)\n        ax.view_init(ax.elev, -120)\n        fig.colorbar(surf)\n        plt.show()\n\n    plot_surface(X, Y, Z_noace, \"{} (No Usable Ace)\".format(title))\n    plot_surface(X, Y, Z_ace, \"{} (Usable Ace)\".format(title))\n\n\n\ndef plot_episode_stats(stats, smoothing_window=10, noshow=False):\n    # Plot the episode length over time\n    fig1 = plt.figure(figsize=(10,5))\n    plt.plot(stats.episode_lengths)\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Episode Length\")\n    plt.title(\"Episode Length over Time\")\n    if noshow:\n        plt.close(fig1)\n    else:\n        plt.show(fig1)\n\n    # Plot the episode reward over time\n    fig2 = plt.figure(figsize=(10,5))\n    rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n    plt.plot(rewards_smoothed)\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Episode Reward (Smoothed)\")\n    plt.title(\"Episode Reward over Time (Smoothed over window size {})\".format(smoothing_window))\n    if noshow:\n        plt.close(fig2)\n    else:\n        plt.show(fig2)\n\n    # Plot time steps and episode number\n    fig3 = plt.figure(figsize=(10,5))\n    plt.plot(np.cumsum(stats.episode_lengths), np.arange(len(stats.episode_lengths)))\n    plt.xlabel(\"Time Steps\")\n    plt.ylabel(\"Episode\")\n    plt.title(\"Episode per time step\")\n    if noshow:\n        plt.close(fig3)\n    else:\n        plt.show(fig3)\n\n    return fig1, fig2, fig3\n",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#%matplotlib inline\n\nimport gym\nimport itertools\nimport matplotlib as lib\nimport numpy as np\nimport sys\nimport sklearn.pipeline\nimport sklearn.preprocessing\n\nif \"../\" not in sys.path:\n  sys.path.append(\"../\") \n\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.kernel_approximation import RBFSampler\n\nmatplotlib.style.use('ggplot')",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "env = gym.envs.make(\"MountainCar-v0\")",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Feature Preprocessing: Normalize to zero mean and unit variance\n# We use a few samples from the observation space to do this\nobservation_examples = np.array([env.observation_space.sample() for x in range(10000)])\nscaler = sklearn.preprocessing.StandardScaler()\nscaler.fit(observation_examples)\n\n# Used to convert a state to a featurizes represenation.\n# We use RBF kernels with different variances to cover different parts of the space\nfeaturizer = sklearn.pipeline.FeatureUnion([\n        (\"rbf1\", RBFSampler(gamma=5.0, n_components=100)),\n        (\"rbf2\", RBFSampler(gamma=2.0, n_components=100)),\n        (\"rbf3\", RBFSampler(gamma=1.0, n_components=100)),\n        (\"rbf4\", RBFSampler(gamma=0.5, n_components=100))\n        ])\nfeaturizer.fit(scaler.transform(observation_examples))",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "FeatureUnion(n_jobs=1,\n       transformer_list=[('rbf1', RBFSampler(gamma=5.0, n_components=100, random_state=None)), ('rbf2', RBFSampler(gamma=2.0, n_components=100, random_state=None)), ('rbf3', RBFSampler(gamma=1.0, n_components=100, random_state=None)), ('rbf4', RBFSampler(gamma=0.5, n_components=100, random_state=None))],\n       transformer_weights=None)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Estimator():\n    \"\"\"\n    Value Function approximator. \n    \"\"\"\n    \n    def __init__(self):\n        # We create a separate model for each action in the environment's\n        # action space. Alternatively we could somehow encode the action\n        # into the features, but this way it's easier to code up.\n        self.models = []\n        for _ in range(env.action_space.n):\n            model = SGDRegressor(learning_rate=\"constant\")\n            # We need to call partial_fit once to initialize the model\n            # or we get a NotFittedError when trying to make a prediction\n            # This is quite hacky.\n            model.partial_fit([self.featurize_state(env.reset())], [0])\n            self.models.append(model)\n    \n    def featurize_state(self, state):\n        \"\"\"\n        Returns the featurized representation for a state.\n        \"\"\"\n        scaled = scaler.transform([state])\n        featurized = featurizer.transform(scaled)\n        return featurized[0]\n    \n    def predict(self, s, a=None):\n        \"\"\"\n        Makes value function predictions.\n        \n        Args:\n            s: state to make a prediction for\n            a: (Optional) action to make a prediction for\n            \n        Returns\n            If an action a is given this returns a single number as the prediction.\n            If no action is given this returns a vector or predictions for all actions\n            in the environment where pred[i] is the prediction for action i.\n            \n        \"\"\"\n        features = self.featurize_state(s)\n        if not a:\n            return np.array([m.predict([features])[0] for m in self.models])\n        else:\n            return self.models[a].predict([features])[0]\n    \n    def update(self, s, a, y):\n        \"\"\"\n        Updates the estimator parameters for a given state and action towards\n        the target y.\n        \"\"\"\n        features = self.featurize_state(s)\n        self.models[a].partial_fit([features], [y])",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def make_epsilon_greedy_policy(estimator, epsilon, nA):\n    \"\"\"\n    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n    \n    Args:\n        estimator: An estimator that returns q values for a given state\n        epsilon: The probability to select a random action . float between 0 and 1.\n        nA: Number of actions in the environment.\n    \n    Returns:\n        A function that takes the observation as an argument and returns\n        the probabilities for each action in the form of a numpy array of length nA.\n    \n    \"\"\"\n    def policy_fn(observation):\n        A = np.ones(nA, dtype=float) * epsilon / nA\n        q_values = estimator.predict(observation)\n        best_action = np.argmax(q_values)\n        A[best_action] += (1.0 - epsilon)\n        return A\n    return policy_fn",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def q_learning(env, estimator, num_episodes, discount_factor=1.0, epsilon=0.1, epsilon_decay=1.0):\n    \"\"\"\n    Q-Learning algorithm for fff-policy TD control using Function Approximation.\n    Finds the optimal greedy policy while following an epsilon-greedy policy.\n    \n    Args:\n        env: OpenAI environment.\n        estimator: Action-Value function estimator\n        num_episodes: Number of episodes to run for.\n        discount_factor: Gamma discount factor.\n        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n        epsilon_decay: Each episode, epsilon is decayed by this factor\n    \n    Returns:\n        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n    \"\"\"\n\n    # Keeps track of useful statistics\n    stats = plot_episode_stats(\n        episode_lengths=np.zeros(num_episodes),\n        episode_rewards=np.zeros(num_episodes))    \n    \n    for i_episode in range(num_episodes):\n        \n        # The policy we're following\n        policy = make_epsilon_greedy_policy(\n            estimator, epsilon * epsilon_decay**i_episode, env.action_space.n)\n        \n        # Print out which episode we're on, useful for debugging.\n        # Also print reward for last episode\n        last_reward = stats.episode_rewards[i_episode - 1]\n        sys.stdout.flush()\n        \n        # Reset the environment and pick the first action\n        state = env.reset()\n        \n        # Only used for SARSA, not Q-Learning\n        next_action = None\n        \n        # One step in the environment\n        for t in itertools.count():\n                        \n            # Choose an action to take\n            # If we're using SARSA we already decided in the previous step\n            if next_action is None:\n                action_probs = policy(state)\n                action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n            else:\n                action = next_action\n            \n            # Take a step\n            next_state, reward, done, _ = env.step(action)\n    \n            # Update statistics\n            stats.episode_rewards[i_episode] += reward\n            stats.episode_lengths[i_episode] = t\n            \n            # TD Update\n            q_values_next = estimator.predict(next_state)\n            \n            # Use this code for Q-Learning\n            # Q-Value TD Target\n            td_target = reward + discount_factor * np.max(q_values_next)\n            \n            # Use this code for SARSA TD Target for on policy-training:\n            # next_action_probs = policy(next_state)\n            # next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)             \n            # td_target = reward + discount_factor * q_values_next[next_action]\n            \n            # Update the function approximator using our target\n            estimator.update(state, action, td_target)\n            \n            print(\"\\rStep {} @ Episode {}/{} ({})\".format(t, i_episode + 1, num_episodes, last_reward), end=\"\")\n                \n            if done:\n                break\n                \n            state = next_state\n    \n    return stats",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "estimator = Estimator()",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Note: For the Mountain Car we don't actually need an epsilon > 0.0\n# because our initial estimate for all states is too \"optimistic\" which leads\n# to the exploration of all states.\nstats = q_learning(env, estimator, 100, epsilon=0.0)",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "plot_episode_stats() got an unexpected keyword argument 'episode_lengths'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a1ad7f1f7f92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# because our initial estimate for all states is too \"optimistic\" which leads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# to the exploration of all states.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-ff119eebf06c>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(env, estimator, num_episodes, discount_factor, epsilon, epsilon_decay)\u001b[0m\n\u001b[1;32m     19\u001b[0m     stats = plot_episode_stats(\n\u001b[1;32m     20\u001b[0m         \u001b[0mepisode_lengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         episode_rewards=np.zeros(num_episodes))    \n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: plot_episode_stats() got an unexpected keyword argument 'episode_lengths'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plotting.plot_cost_to_go_mountain_car(env, estimator)\nplotting.plot_episode_stats(stats, smoothing_window=25)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}