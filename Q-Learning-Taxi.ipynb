{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Q-Learning (Taxi-v2)\n\nIn this notebook I present an implementation of the Q-learning algorithm and apply it to the \"Taxi-v2\" AI Gym Environment in roder to obtain the optimal Q-table values for the problem. \n\n### Dependencies"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from IPython.display import clear_output\nimport numpy as np\nimport random\nimport time\nimport gym",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Envinroment: [Taxi-v2](https://gym.openai.com/envs/Taxi-v2/)\n\nThis task was introduced in [Dietterich2000](https://dl.acm.org/citation.cfm?id=1622268) to illustrate some issues in hierarchical reinforcement learning. There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "env = gym.make(\"Taxi-v2\") # Create environment\nenv.render() # Show it",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+---------+\n|R: | : :\u001b[34;1mG\u001b[0m|\n| : : : : |\n| : : : : |\n| | : | : |\n|\u001b[35mY\u001b[0m|\u001b[43m \u001b[0m: |B: |\n+---------+\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Q-table\n\nIn [*Q-learning*](https://en.wikipedia.org/wiki/Q-learning) the goal is to learn a policy that will tell an agent which action to take under each possible state. The *Q-table* is responsible to store **score** values for each *(state, action)* pair. These values can be initialized with zeros or randomly, and them they are updated as you perform *exploration* in your problem domain (which helps to discover which actions leads to a better stream of rewards)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Number of possible actions\naction_size = env.action_space.n \nprint(\"Action size \", action_size) \n\n# Number of possible states\nstate_size = env.observation_space.n \nprint(\"State size \", state_size)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Action size  6\nState size  500\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "qtable = np.zeros((state_size, action_size))\nprint(qtable)",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]\n ...\n [0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Hyper-parameters"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "episodes = 30000            # Total episodes\nmax_steps = 1000            # Max steps per episode\nlr = 0.3                    # Learning rate\ndecay_fac = 0.00001         # Decay learning rate each iteration\ngamma = 0.90                # Discounting rate - later rewards impact less",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exploration (Q-learning)\n\nSince **the goal here is to learn the optimal q-table**, that is, best policy for each (state,action) pair: we need to perform exploration only. If the goal was to learn and solve the environment as quickly as possible, it would be necessary to implement an [exploration-exploitation](http://home.deib.polimi.it/restelli/MyWebSite/pdf/rl5.pdf) strategy. This kind of strategy would not work for this goal since we want to discover the optimal values for states that would not often/never be visited via the exploration-exploitation strategy.\n\nThe exploration strategy here implemented is straightforward: for each state we get to, *take an action randomly*. The randomness \"guarantees\" that we are going to visit each state action pair eventually."
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "for episode in range(episodes):\n    \n    state = env.reset() # Reset the environment\n    done = False        # Are we done with the environment\n    lr -= decay_fac     # Decaying learning rate\n    step = 0\n    \n    if lr <= 0: # Nothing more to learn?\n        break\n        \n    for step in range(max_steps):\n        \n        # Randomly Choose an Action\n        action = env.action_space.sample()\n        \n        # Take the action -> observe new state and reward\n        new_state, reward, done, info = env.step(action)\n        \n        # Update qtable values\n        if done == True: # If last, do not count future accumulated reward\n            if(step < 199 | step > 201):\n                qtable[state, action] = qtable[state, action]+lr*(reward+gamma*0-qtable[state,action])\n            break\n        else: # Consider accumulated reward of best decision stream\n            qtable[state, action] = qtable[state,action]+lr*(reward+gamma*np.max(qtable[new_state,:])-qtable[state,action])\n    \n        # if done.. jump to next episode\n        if done == True:\n            break\n        \n        # moving states\n        state = new_state\n        \n    episode += 1\n    \n    if (episode % 3000 == 0):\n        print('episode = ', episode)\n        print('learning rate = ', lr)\n        print('-----------')\n",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "episode =  3000\nlearning rate =  0.26999999999997\n-----------\nepisode =  6000\nlearning rate =  0.23999999999993998\n-----------\nepisode =  9000\nlearning rate =  0.20999999999990998\n-----------\nepisode =  12000\nlearning rate =  0.17999999999987998\n-----------\nepisode =  15000\nlearning rate =  0.14999999999984998\n-----------\nepisode =  18000\nlearning rate =  0.11999999999982693\n-----------\nepisode =  21000\nlearning rate =  0.08999999999983856\n-----------\nepisode =  24000\nlearning rate =  0.059999999999848445\n-----------\nepisode =  27000\nlearning rate =  0.029999999999839697\n-----------\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Using Q-table\n\nNow that we have the optimal values in the Q-table, we can use it to see our agent taking the best actions in this setting.\nYou can re-run the code below to see it solving different environments."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# New environment\nstate = env.reset()\nenv.render()\ndone = False\ntotal_reward = 0\n\nwhile(done == False):\n    \n    action = np.argmax(qtable[state,:]) # Choose best action (Q-table)\n    state, reward, done, info = env.step(action) # Take action\n    total_reward += reward  # Summing rewards\n    \n    # Display it\n    time.sleep(0.5)\n    clear_output(wait=True)\n    env.render()\n    print('Episode Reward = ', total_reward)\n    ",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+---------+\n|R: | : :G|\n| : : : : |\n| : : : : |\n| | : | : |\n|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n+---------+\n  (Dropoff)\nEpisode Reward =  7\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}